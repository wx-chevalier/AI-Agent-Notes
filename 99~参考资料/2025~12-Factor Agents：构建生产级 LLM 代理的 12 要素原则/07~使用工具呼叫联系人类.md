# 使用工具呼叫联系人类

虽然 LLM 功能强大，但总有需要人类判断、专业知识或干预的场景。本原则主张通过工具调用，将"人机交互"明确纳入代理功能。当 LLM 遇到模糊、不确定或需外部批准的情况时，应能触发结构化通知或上报给人类操作员，确保关键决策在需要时获得人工监督。

![Contact Human with tools](https://apframework.com/static/images/2025-07-08-12-Factor-Agents/image%206.png)

默认情况下，LLM API 依赖于关键的"高风险"令牌选择：我们是返回纯文本，还是结构化数据？

你对第一个标记的选择非常重视，在这种 `the weather in tokyo` 情况下，它是：

> 这

但在 `fetch_weather` 情况下，它是一些特殊标记，表示 JSON 对象的开始：

> JSON>

让 LLM 始终输出 json，并用自然语言标记（如 `request_human_input` 或 `done_for_now`，而不是像 `check_weather_in_city` 这样的具体工具）声明意图，往往能获得更好效果。

不会直接提升性能，但你应大胆实验，确保有自由尝试各种方式以获得最佳结果。

```go
class Options:
  urgency: Literal["low", "medium", "high"]
  format: Literal["free_text", "yes_no", "multiple_choice"]
  choices: List[str]

# 人机交互工具定义
class RequestHumanInput:
  intent: "request_human_input"
  question: str
  context: str
  options: Options

# 代理循环中的用法示例
if nextStep.intent == 'request_human_input':
  thread.events.append({
    type: 'human_input_requested',
    data: nextStep
  })
  thread_id = await save_state(thread)
  await notify_human(nextStep, thread_id)
  return
  # 跳出循环，等待带 thread ID 的响应返回
else:
  # ... 其他情况
```

稍后，你可能会从处理 slack、邮件、短信等事件的系统收到 webhook：

```py
@app.post('/webhook')
def webhook(req: Request):
  thread_id = req.body.threadId
  thread = await load_state(thread_id)
  thread.events.push({
    type: 'response_from_human',
    data: req.body
  })
  # ... 简化处理，实际不建议阻塞 web worker
  next_step = await determine_next_step(thread_to_prompt(thread))
  thread.events.append(next_step)
  result = await handle_next_step(thread, next_step)
  # todo - 循环、跳出或其他自定义逻辑

  return {"status": "ok"}
```

以上包括因素 5（统一执行状态和业务状态）、因素 8（拥有自己的控制流）、因素 3（拥有自己的上下文窗口）和因素 4（工具只是结构化输出）的模式，以及其他几个。

如果我们使用因素 3 - 用自定义格式化的上下文窗口，几轮后上下文可能如下：

```xml
(snipped for brevity)

<slack_message>
    From: @alex
    Channel: #deployments
    Text: Can you deploy backend v1.2.3 to production?
    Thread: []
</slack_message>

<request_human_input>
    intent: "request_human_input"
    question: "Would you like to proceed with deploying v1.2.3 to production?"
    context: "This is a production deployment that will affect live users."
    options: {
        urgency: "high"
        format: "yes_no"
    }
</request_human_input>

<human_response>
    response: "yes please proceed"
    approved: true
    timestamp: "2024-03-15T10:30:00Z"
    user: "alex@company.com"
</human_response>

<deploy_backend>
    intent: "deploy_backend"
    tag: "v1.2.3"
    environment: "production"
</deploy_backend>

<deploy_backend_result>
    status: "success"
    message: "Deployment v1.2.3 to production completed successfully."
    timestamp: "2024-03-15T10:30:00Z"
</deploy_backend_result>
```

这样做的好处：

1. 明确指示：针对不同类型人机交互的工具让 LLM 输出更具体
2. 内外循环：支持在传统 ChatGPT 风格界面之外的代理流程，控制流和上下文初始化不必总是 Agent->Human
3. 多人协作：结构化事件便于跟踪和协调多方输入
4. 多代理：简单抽象可扩展为 Agent->Agent 请求与响应
5. 持久性：结合简单 API 启动/暂停/恢复，打造持久、可靠、可自省的多方工作流

![](https://apframework.com/static/images/2025-07-08-12-Factor-Agents/image%207.png)

# **理解笔记**

构建 Agent 过程中，将"人机交互"标签，明确地加入到代理的功能。

- 明确将"请求人工输入"作为一种标准工具调用，便于 LLM 在不确定或高风险场景下主动寻求人类协助。
- 为人机交互设计结构化事件和回调机制，支持多渠道通知和响应。
- 为人工干预流程建立审计和追踪，提升系统安全性和合规性。
